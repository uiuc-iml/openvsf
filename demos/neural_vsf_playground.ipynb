{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#if you haven't installed vsf and just want to run this from the current directory, uncomment the following lines\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the demo data from box (130MB): \n",
    "+ #### Demo data: https://uofi.box.com/s/31wozq63qgqvg8012r1jdfj5mdchmmfp\n",
    "\n",
    "### Demo data include 6 objects with heterogeneous stiffness:  \n",
    "\n",
    "- **brown_boot_moving**:  \n",
    "    - **Description**: a boot placed freely on the table. Punyo sensor + kinova robot arm is used to collect tactile data. Use FoundationPose to track 6D pose of the object.  \n",
    "    - **Support sensor(s)**: punyo dense force (suggested), punyo pressure, joint torque.  \n",
    "    - **VSF initialized from**: object mesh  \n",
    "- **brown_boot_fixed**:  \n",
    "    - **Description**: a boot mounted on the table. Punyo sensor + kinova robot arm is used to collect tactile data.  \n",
    "    - **Support sensor(s)**: punyo dense force (suggested), punyo pressure, joint torque.  \n",
    "    - **VSF initialized from**: object RGBD image (suggested), object bounding box  \n",
    "- **gray_shoe_fixed**:  \n",
    "    - same as above  \n",
    "- **white_nike_fixed**:  \n",
    "    - same as above  \n",
    "- **rubber_fig_tall_angle00**:  \n",
    "    - **Description**: an artificial tree placed on the ground. Use kinova arm to push the tree to collect the tactile data.  \n",
    "    - **Support sensor(s)**: joint torque.  \n",
    "    - **VSF initialized from**: object RGBD image  \n",
    "- **rubber_fig_tall_angle06**:  \n",
    "    - same as above  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize VSF\n",
    "\n",
    "We have already estimated some neural VSFs for you in the demo dataset.  To visualize them, you can use the `vsf_show` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# demo data directory\n",
    "DEMO_DIR = \"../demo_data\" # change to your path\n",
    "\n",
    "OBJECT = \"brown_boot_fixed\"\n",
    "DATA_DIR = os.path.join(DEMO_DIR, 'datasets', OBJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize VSF\n",
    "from vsf.constructors import vsf_from_file\n",
    "import torch\n",
    "\n",
    "#estimated VSFs have been saved to the \"outputs\" folder\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vsf = vsf_from_file(os.path.join(DEMO_DIR, \"saved_vsfs\", \"brown_boot_moving\", \"neural_vsf.pt\")).to(device)\n",
    "\n",
    "from klampt import vis\n",
    "from vsf.visualize.klampt_visualization import vsf_show\n",
    "vis.init('PyQt5')  #needed inside Jupyter Notebook to show an OpenGL window\n",
    "vsf_show(vsf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walkthrough neural VSF training and visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a world and simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import klampt\n",
    "from klampt.math import se3\n",
    "from vsf.sim import QuasistaticVSFSimulator\n",
    "from vsf.sensor.punyo_dense_force_sensor import PunyoDenseForceSensor\n",
    "from vsf.sim.klampt_world_wrapper import klamptWorldWrapper\n",
    "from vsf.utils.klampt_utils import load_trimesh_preserve_vertices\n",
    "\n",
    "# NOTE: This part of the code shows how to manually setup a simulation world with a robot, a tactile sensor and a deformable object,\n",
    "#       and how to estimate the VSF from tactile data.\n",
    "#       A script to automate this process from config files is in scripts/neural_vsf_estimate.py\n",
    "\n",
    "# create a world for simulation and visualization\n",
    "world = klamptWorldWrapper()\n",
    "\n",
    "# add robot/mesh to the world\n",
    "world.add_robot('kinova', os.path.join('../knowledge/robot_model', 'kinova_gen3.urdf'))\n",
    "robot = world.world.robot(0)\n",
    "ee_link = robot.link(robot.numLinks()-1)\n",
    "punyo2ee = np.array(json.load(open(os.path.join('../knowledge/robot_model', 'punyo2end_effector_transform.json')))['punyo2ee'])\n",
    "\n",
    "#this function should be used instead of native Klampt loaders due to a known Assimp configuration issue\n",
    "mesh = load_trimesh_preserve_vertices(os.path.join('../knowledge/robot_model', 'punyo_mesh_partial.ply'))\n",
    "world.add_geometry('punyo', mesh, 'deformable', ee_link.getName(), punyo2ee)\n",
    "\n",
    "# adds the VSF object to the world, for visualization purposes\n",
    "# add object to the visualization without interfering with the simulator\n",
    "object = klampt.Geometry3D()\n",
    "\n",
    "# visualize the object with a heightmap (projected from RGB-D images)\n",
    "import imageio\n",
    "rgb_image = imageio.imread(os.path.join(DATA_DIR, \"object\", \"color_img.jpg\"))\n",
    "depth_image = imageio.imread(os.path.join(DATA_DIR, \"object\", \"depth_img.png\"))\n",
    "depth_scale = 1000.0\n",
    "depth_trunc = 2.0\n",
    "\n",
    "intrinsic = json.load(open(os.path.join(DATA_DIR, \"object\", \"intrinsic.json\")))\n",
    "extrinsic = json.load(open(os.path.join(DATA_DIR, \"object\", \"extrinsic.json\")))\n",
    "extrinsic = np.array(extrinsic['cam2world'])\n",
    "\n",
    "bmin, bmax = np.load(os.path.join(DATA_DIR, \"object\", \"aabb.npy\"))\n",
    "vp = klampt.Viewport()\n",
    "vp.setPose(*se3.from_homogeneous(extrinsic))\n",
    "vp.w, vp.h = rgb_image.shape[1], rgb_image.shape[0]\n",
    "vp.fx, vp.fy = intrinsic['fx'], intrinsic['fy']\n",
    "vp.cx, vp.cy = intrinsic['cx'], intrinsic['cy']\n",
    "\n",
    "# remove points beyond the truncation distance    \n",
    "depth_image[depth_image > depth_trunc*depth_scale] = 0\n",
    "\n",
    "# create a heightmap from the rgbd image\n",
    "hm_data = klampt.Heightmap()\n",
    "hm_data.setViewport(vp)\n",
    "hm_data.setHeightImage(depth_image, 1/depth_scale)\n",
    "hm_data.setColorImage(rgb_image)\n",
    "object.setHeightmap(hm_data)\n",
    "object = object.convert('PointCloud')\n",
    "\n",
    "# initialize sensors\n",
    "sensors = [PunyoDenseForceSensor('punyo_force', 'punyo')] # add sensor named 'punyo_force' and attach it to the mesh named 'punyo'\n",
    "\n",
    "# create simulator\n",
    "sim = QuasistaticVSFSimulator(world, sensors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tactile dataset and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dacite\n",
    "from vsf.dataset.constructors import dataset_from_config, DatasetConfig\n",
    "\n",
    "from vsf.utils.config_utils import load_config_recursive\n",
    "dataset_config = load_config_recursive(os.path.join('../demo_data/datasets/punyo_dataset_config.yaml'))\n",
    "dataset_config = dacite.from_dict(DatasetConfig, dataset_config)\n",
    "dataset_config.path = os.path.join('../demo_data/datasets', OBJECT)\n",
    "dataset = dataset_from_config(dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from klampt import vis\n",
    "vis.add(\"world\", world.world)\n",
    "\n",
    "# add table\n",
    "# the table are for visualization only, not added to the simulator to avoid extra computation in the simulation\n",
    "from klampt.model.create import box\n",
    "b1 = box(1.,1.5,1.1,center=(0.25,0,-0.55),type='GeometricPrimitive')\n",
    "vis.add(\"table\", b1, hide_label=True)\n",
    "b2 = box(0.7,0.45,0.12,center=(0.3,0.4,0),type='GeometricPrimitive')\n",
    "vis.add(\"box\", b2, hide_label=True)\n",
    "\n",
    "# add visualization object to show its estimated pose\n",
    "vis.add(\"object\", object)\n",
    "vis.show()\n",
    "\n",
    "import time\n",
    "for i in range(len(dataset)):\n",
    "    seq = dataset[i]\n",
    "    for frame in seq:\n",
    "        control = {}\n",
    "        control['kinova'] = frame['kinova_state']\n",
    "        control['punyo'] = frame['punyo_state']\n",
    "\n",
    "        # step simulation\n",
    "        vis.lock()\n",
    "        sim.step(control, 0.1)\n",
    "        vis.unlock()\n",
    "\n",
    "        time.sleep(2/len(seq)) # visualize each sequence for 2 second\n",
    "\n",
    "    sim.reset()\n",
    "    if i >= 5:\n",
    "        break\n",
    "vis.scene().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create VSF model\n",
    "import torch\n",
    "from vsf.constructors import vsf_from_box\n",
    "\n",
    "# create vsf from a bounding box\n",
    "aabb = np.load(os.path.join(DATA_DIR, \"object\", \"aabb.npy\"))\n",
    "vsf = vsf_from_box(aabb[0], aabb[1], type='neural')\n",
    "\n",
    "# move vsf to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vsf = vsf.to(device)\n",
    "\n",
    "# add vsf to the scene\n",
    "sim.add_deformable('vsf',vsf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run estimation\n",
    "\n",
    "The following code runs a batch estimation over the whole dataset.  Note that training is performed by randomizing over sequences in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some sensors need to be calibrated before use\n",
    "# for PunyoDenseForceSensor, no calibration is needed\n",
    "calibrators = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch VSF estimation demo\n",
    "from vsf.estimator.neural_vsf_estimator import NeuralVSFEstimator, NeuralVSFEstimatorConfig\n",
    "\n",
    "# create estimator\n",
    "estimator_config = NeuralVSFEstimatorConfig(lr=2e-4,\n",
    "                                            regularizer_samples=500,\n",
    "                                            regularizer_scale=1e-8,\n",
    "                                            max_epochs=500,\n",
    "                                            down_sample_rate=1)\n",
    "                                            \n",
    "estimator = NeuralVSFEstimator(estimator_config)\n",
    "\n",
    "print(\"Starting batch estimation, using device\",vsf.device)\n",
    "estimator.batch_estimate(sim, vsf, dataset, dataset_config, calibrators)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code runs an online estimation for each sequence in the dataset.  This does have a risk of catastrophic forgetting, since there is no memory replay buffer (as in Point VSF online estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online VSF estimation demo\n",
    "from vsf.utils.data_utils import remap_dict_in_seq\n",
    "# create estimator\n",
    "estimator_config = NeuralVSFEstimatorConfig(lr=1e-3,\n",
    "                                            regularizer_samples=500,\n",
    "                                            regularizer_scale=1e-8,\n",
    "                                            down_sample_rate=1)\n",
    "estimator = NeuralVSFEstimator(estimator_config)\n",
    "\n",
    "estimator.online_init(sim, vsf)\n",
    "\n",
    "dt = 0.1\n",
    "for i in range(len(dataset)):\n",
    "    print(\"Beginning sequence\",i)\n",
    "    seq = dataset[i]\n",
    "    sim.reset()\n",
    "    estimator.online_reset(sim)\n",
    "\n",
    "    control_keys = sim.get_control_keys()\n",
    "    sensor_keys = sim.get_sensor_keys()\n",
    "    control_seq, observation_seq = remap_dict_in_seq(seq, control_keys, sensor_keys)\n",
    "\n",
    "    #calibrate the sensors from the sequences\n",
    "    ncalibrate = 0\n",
    "    if calibrators is not None and calibrators != {}:\n",
    "        for k,v in calibrators.items():\n",
    "            sensor = sim.get_sensor(k)\n",
    "            if sensor is None: raise ValueError(f\"Sensor {k} not found in simulator\")\n",
    "            n = v.calibrate(sensor, sim, control_seq, observation_seq)\n",
    "            ncalibrate = max(n,ncalibrate)\n",
    "\n",
    "\n",
    "    for i in range(ncalibrate, len(seq), estimator_config.down_sample_rate):\n",
    "        controls = control_seq[i]\n",
    "        observations = observation_seq[i]\n",
    "        sim.step(controls,dt)\n",
    "    \n",
    "        estimator.online_update(sim, dt, observations)\n",
    "\n",
    "estimator.online_finalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize VSF\n",
    "from vsf.visualize.klampt_visualization import vsf_show\n",
    "vsf_show(vsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk        \n",
    "os.makedirs(os.path.join(DEMO_DIR, \"saved_vsfs\", OBJECT), exist_ok=True)\n",
    "vsf.save(os.path.join(DEMO_DIR, \"saved_vsfs\", OBJECT, \"neural_vsf_playground.pt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different VSF initialization method and VSF pose control\n",
    "\n",
    "VSF can be initialized from different object information. The above example uses a simple bounding box to initialize a Neural VSF. Other initialization methods provided including initializing from object mesh / object RGBD image. These two methods eliminate empty space, so can result in better training speed and model quality. \n",
    "\n",
    "Pose control for VSF is supported in simulator. This allows the target object to be moving while collecting the data.\n",
    "\n",
    "Here is an example for reconstructing Nerual VSF for an boot lying freely on a table, with object mesh for VSF initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECT = \"brown_boot_moving\"\n",
    "DATA_DIR = os.path.join(DEMO_DIR, 'datasets', OBJECT)\n",
    "print(\"Loading dataset from\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = klamptWorldWrapper()\n",
    "\n",
    "# add robot/mesh to the world\n",
    "world.add_robot('kinova', os.path.join('../knowledge/robot_model', 'kinova_gen3.urdf'))\n",
    "robot = world.world.robot(0)\n",
    "ee_link = robot.link(robot.numLinks()-1)\n",
    "punyo2ee = np.array(json.load(open(os.path.join('../knowledge/robot_model', 'punyo2end_effector_transform.json')))['punyo2ee'])\n",
    "\n",
    "#this function should be used instead of native Klampt loaders due to a known Assimp configuration issue\n",
    "mesh = load_trimesh_preserve_vertices(os.path.join('../knowledge/robot_model', 'punyo_mesh_partial.ply'))\n",
    "world.add_geometry('punyo', mesh, 'deformable', ee_link.getName(), punyo2ee)\n",
    "\n",
    "# load the object mesh , for visualization purposes\n",
    "object = klampt.Geometry3D()\n",
    "object.loadFile(os.path.join(DATA_DIR, \"object\", \"mesh.obj\"))\n",
    "\n",
    "# initialize sensors\n",
    "sensors = [PunyoDenseForceSensor('punyo_force', 'punyo')] # add sensor named 'punyo_force' and attach it to the mesh named 'punyo'\n",
    "\n",
    "\n",
    "# create simulator\n",
    "# use the same world and sensors config as before\n",
    "sim = QuasistaticVSFSimulator(world, sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config.path = os.path.join('../demo_data/datasets', OBJECT)\n",
    "dataset = dataset_from_config(dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from klampt import vis\n",
    "vis.add(\"world\", world.world)\n",
    "\n",
    "\n",
    "# add table\n",
    "# the table are for visualization only, not added to the simulator to avoid extra computation in the simulation\n",
    "from klampt.model.create import box\n",
    "b1 = box(1.,1.5,1.1,center=(0.25,0,-0.55),type='GeometricPrimitive')\n",
    "vis.add(\"table\", b1, hide_label=True)\n",
    "b2 = box(0.7,0.45,0.12,center=(0.3,0.4,0),type='GeometricPrimitive')\n",
    "vis.add(\"box\", b2, hide_label=True)\n",
    "\n",
    "# add visualization object to show its estimated pose\n",
    "vis.add(\"object\", object)\n",
    "vis.show()\n",
    "\n",
    "import time\n",
    "for i in range(len(dataset)):\n",
    "    seq = dataset[i]\n",
    "    for frame in seq:\n",
    "        control = {}\n",
    "        control['kinova'] = frame['kinova_state']\n",
    "        control['punyo'] = frame['punyo_state']\n",
    "\n",
    "        # step simulation\n",
    "        vis.lock()\n",
    "        sim.step(control, 0.1)\n",
    "        object.setCurrentTransform(*se3.from_ndarray(frame['vsf_state'])) # object mesh is for visualization only, so not added to the simulator\n",
    "        vis.unlock()\n",
    "\n",
    "        time.sleep(2/len(seq)) # visualize each sequence for 2 second\n",
    "\n",
    "    sim.reset()\n",
    "    if i >= 5:\n",
    "        break\n",
    "vis.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vsf.constructors import vsf_from_mesh\n",
    "\n",
    "# create vsf from a mesh -- this will do a better job estimating stiffness at the object boundaries\n",
    "vsf = vsf_from_mesh(os.path.join(DATA_DIR, \"object\", \"mesh.obj\"))\n",
    "\n",
    "# move vsf to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vsf = vsf.to(device)\n",
    "\n",
    "# add vsf to the scene\n",
    "sim.add_deformable('vsf',vsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same estimator config as before\n",
    "print(\"Starting batch estimation, using device\",vsf.device)\n",
    "estimator.batch_estimate(sim, vsf, dataset, dataset_config, calibrators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize VSF\n",
    "from vsf.visualize.klampt_visualization import vsf_show\n",
    "vsf_show(vsf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural VSF to Point VSF\n",
    "\n",
    "Neural VSF can be converted to Point VSF. The following code shows how to convert a neural VSF to a Point VSF. The Point VSF is initialized by sampling points from the neural VSF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vsf.constructors import vsf_from_vsf, PointVSFConfig\n",
    "\n",
    "voxel_size = 3e-3\n",
    "point_config = PointVSFConfig(voxel_size=voxel_size)\n",
    "point_vsf = vsf_from_vsf(point_config, vsf)\n",
    "vsf_show(point_vsf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point VSF to Neural VSF\n",
    "\n",
    "Similarly, Point VSF can be converted to Neural VSF. The following code shows how to convert a Point VSF to a Neural VSF. The neural VSF is trained by distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vsf.constructors import vsf_from_vsf, NeuralVSFConfig\n",
    "\n",
    "neural_config = NeuralVSFConfig()\n",
    "\n",
    "convert_config = NeuralVSFEstimatorConfig(lr=2e-4,\n",
    "                                          regularizer_samples=1000,\n",
    "                                          regularizer_scale=1e2,\n",
    "                                          max_epochs=500)\n",
    "neural_vsf = vsf_from_vsf(neural_config, point_vsf, convert_config)\n",
    "vsf_show(neural_vsf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
