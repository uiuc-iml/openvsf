{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#if you haven't installed vsf and just want to run this from the current directory, uncomment the following lines\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the demo data from box (80MB): \n",
    "+ #### Demo data: https://uofi.box.com/s/31wozq63qgqvg8012r1jdfj5mdchmmfp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize VSF\n",
    "\n",
    "We have already estimated some neural VSFs for you in the demo dataset.  To visualize them, you can use the `vsf_show` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# demo data directory\n",
    "DEMO_DIR = \"../demo_data\" # change to your path\n",
    "OBJECT = \"brown_boot_moving\" # choose from \"brown_boot_fixed\" or \"brown_boot_moving\"\n",
    "# OBJECT = \"rubber_fig_tall_angle00\" # choose from \"brown_boot_fixed\" or \"brown_boot_moving\"\n",
    "DATA_DIR = os.path.join(DEMO_DIR, 'datasets', OBJECT)\n",
    "TRIAL = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# visualize VSF\n",
    "from vsf.constructors import vsf_from_file\n",
    "import torch\n",
    "\n",
    "#estimated VSFs have been saved to the \"outputs\" folder\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vsf = vsf_from_file(os.path.join(DEMO_DIR, \"saved_vsfs\", OBJECT, \"neural_vsf.pt\")).to(device)\n",
    "\n",
    "from klampt import vis\n",
    "from vsf.visualize.klampt_visualization import vsf_show\n",
    "vis.init('PyQt')  #needed inside Jupyter Notebook to show an OpenGL window\n",
    "vsf_show(vsf)\n",
    "vis.clear()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walkthrough neural VSF training and visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a world and simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WorldModel::LoadRobot: ../knowledge/robot_model/kinova_gen3.urdf\n",
      "URDFParser: Link size: 9\n",
      "URDFParser: Joint size: 9\n",
      "URDFParser: Done loading robot file ../knowledge/robot_model/kinova_gen3.urdf\n",
      "ManagedGeometry: loaded ../demo_data/datasets/brown_boot_moving/object/mesh.obj in time 0.648093s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import klampt\n",
    "from klampt.math import se3\n",
    "from vsf.sim import QuasistaticVSFSimulator\n",
    "from vsf.sensor.joint_torque_sensor import JointTorqueSensor\n",
    "from vsf.sensor.punyo_dense_force_sensor import PunyoDenseForceSensor\n",
    "from vsf.sim.klampt_world_wrapper import klamptWorldWrapper\n",
    "from vsf.utils.klampt_utils import load_trimesh_preserve_vertices\n",
    "\n",
    "# NOTE: This part of the code shows how to manually setup a simulation world with a robot, a tactile sensor and a deformable object,\n",
    "#       and how to estimate the VSF from tactile data.\n",
    "#       A script to automate this process from config files is in scripts/neural_vsf_estimate.py\n",
    "\n",
    "# create a world for simulation and visualization\n",
    "world = klamptWorldWrapper()\n",
    "\n",
    "# add robot/mesh to the world\n",
    "world.add_robot('kinova', os.path.join('../knowledge/robot_model', 'kinova_gen3.urdf'))\n",
    "robot = world.world.robot(0)\n",
    "ee_link = robot.link(robot.numLinks()-1)\n",
    "punyo2ee = np.array(json.load(open(os.path.join('../knowledge/robot_model', 'punyo2end_effector_transform.json')))['punyo2ee'])\n",
    "\n",
    "#this function should be used instead of native Klampt loaders due to a known Assimp configuration issue\n",
    "mesh = load_trimesh_preserve_vertices(os.path.join('../knowledge/robot_model', 'punyo_mesh_partial.ply'))\n",
    "world.add_geometry('punyo', mesh, 'deformable', ee_link.getName(), punyo2ee)\n",
    "\n",
    "# adds the VSF object to the world, for visualization purposes\n",
    "# hack for add object to the visualization without interfering with the simulator\n",
    "world2 = klampt.WorldModel()\n",
    "object = world2.makeRigidObject(\"object\")\n",
    "if os.path.exists(os.path.join(DATA_DIR, \"object\", \"mesh.obj\")):\n",
    "    object.geometry().loadFile(os.path.join(DATA_DIR, \"object\", \"mesh.obj\"))\n",
    "elif os.path.exists(os.path.join(DATA_DIR, \"object\", \"color_img.jpg\")) and os.path.exists(os.path.join(DATA_DIR, \"object\", \"depth_img.png\")):\n",
    "    # visualize the object with a heightmap (projected from RGB-D images)\n",
    "    import imageio\n",
    "    rgb_image = imageio.imread(os.path.join(DATA_DIR, \"object\", \"color_img.jpg\"))\n",
    "    depth_image = imageio.imread(os.path.join(DATA_DIR, \"object\", \"depth_img.png\"))\n",
    "    depth_scale = 1000.0\n",
    "    depth_trunc = 2.0\n",
    "\n",
    "    intrinsic = json.load(open(os.path.join(DATA_DIR, \"object\", \"intrinsic.json\")))\n",
    "    extrinsic = json.load(open(os.path.join(DATA_DIR, \"object\", \"extrinsic.json\")))\n",
    "    extrinsic = np.array(extrinsic['cam2world'])\n",
    "\n",
    "    bmin, bmax = np.load(os.path.join(DATA_DIR, \"object\", \"aabb.npy\"))\n",
    "    vp = klampt.Viewport()\n",
    "    vp.setPose(*se3.from_homogeneous(extrinsic))\n",
    "    vp.w, vp.h = rgb_image.shape[1], rgb_image.shape[0]\n",
    "    vp.fx, vp.fy = intrinsic['fx'], intrinsic['fy']\n",
    "    vp.cx, vp.cy = intrinsic['cx'], intrinsic['cy']\n",
    "\n",
    "    # remove points beyond the truncation distance    \n",
    "    depth_image[depth_image > depth_trunc*depth_scale] = 0\n",
    "\n",
    "    # create a heightmap from the rgbd image\n",
    "    hm_data = klampt.Heightmap()\n",
    "    hm_data.setViewport(vp)\n",
    "    hm_data.setHeightImage(depth_image, 1/depth_scale)\n",
    "    hm_data.setColorImage(rgb_image)\n",
    "    object.geometry().setHeightmap(hm_data)\n",
    "    \n",
    "else:\n",
    "    aabb = np.load(os.path.join(DATA_DIR, \"object\", \"aabb.npy\"))\n",
    "    object.geometry().setTriangleMesh(klampt.model.create.primitives.bbox(*aabb).getTriangleMesh())\n",
    "    \n",
    "\n",
    "# initialize sensors\n",
    "sensors = [PunyoDenseForceSensor('punyo_force', 'punyo')] # add sensor named 'punyo_force' and attach it to the mesh named 'punyo'\n",
    "# sensors = [JointTorqueSensor('kinova_joint_torques','kinova',[robot.link(i).name for i in range(1,8)])]\n",
    "\n",
    "# create simulator\n",
    "sim = QuasistaticVSFSimulator(world, sensors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tactile dataset and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dacite\n",
    "from vsf.dataset.constructors import dataset_from_config, DatasetConfig\n",
    "\n",
    "from vsf.utils.config_utils import load_config_recursive\n",
    "# dataset_config = load_config_recursive(os.path.join('../demo_data/datasets/joint_torques_dataset_config.yaml'))\n",
    "dataset_config = load_config_recursive(os.path.join('../demo_data/datasets/punyo_dataset_config.yaml'))\n",
    "dataset_config = dacite.from_dict(DatasetConfig, dataset_config)\n",
    "dataset_config.path = os.path.join('../demo_data/datasets', OBJECT, TRIAL)\n",
    "dataset = dataset_from_config(dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***  klampt.vis: using Qt6 as the visualization backend  ***\n",
      "Loading sequence ../demo_data/datasets/brown_boot_moving/seq_000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: QApplication was not created in the main() thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vis: creating GL window\n",
      "######### QtGLWindow setProgram ###############\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "######### QtGLWindow Initialize GL ###############\n",
      "QtGLWindow.resizeGL: called when invisible?\n",
      "QtGLWindow.paintGL: called while invisible?\n",
      "TriMeshTopology: mesh has 9 triangles with duplicate neighbors!\n",
      "  Triangle range 273 to 924\n",
      "  May see strange results for some triangle mesh operations\n",
      "TriMeshTopology: mesh has 104 triangles with duplicate neighbors!\n",
      "  Triangle range 3828 to 4274\n",
      "  May see strange results for some triangle mesh operations\n",
      "TriMeshTopology: mesh has 271 triangles with duplicate neighbors!\n",
      "  Triangle range 80 to 3362\n",
      "  May see strange results for some triangle mesh operations\n",
      "TriMeshTopology: mesh has 635 triangles with duplicate neighbors!\n",
      "  Triangle range 29 to 3626\n",
      "  May see strange results for some triangle mesh operations\n",
      "TriMeshTopology: mesh has 587 triangles with duplicate neighbors!\n",
      "  Triangle range 1988 to 3472\n",
      "  May see strange results for some triangle mesh operations\n",
      "TriMeshTopology: mesh has 344 triangles with duplicate neighbors!\n",
      "  Triangle range 4000 to 5015\n",
      "  May see strange results for some triangle mesh operations\n",
      "TriMeshTopology: mesh has 344 triangles with duplicate neighbors!\n",
      "  Triangle range 3199 to 4281\n",
      "  May see strange results for some triangle mesh operations\n",
      "TriMeshTopology: mesh has 378 triangles with duplicate neighbors!\n",
      "  Triangle range 2 to 8834\n",
      "  May see strange results for some triangle mesh operations\n",
      "Loading sequence ../demo_data/datasets/brown_boot_moving/seq_001\n",
      "Loading sequence ../demo_data/datasets/brown_boot_moving/seq_002\n",
      "Loading sequence ../demo_data/datasets/brown_boot_moving/seq_003\n",
      "Loading sequence ../demo_data/datasets/brown_boot_moving/seq_004\n",
      "Loading sequence ../demo_data/datasets/brown_boot_moving/seq_005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n"
     ]
    }
   ],
   "source": [
    "from klampt import vis\n",
    "vis.init('PyQt')  #needed inside Jupyter Notebook to show an OpenGL window\n",
    "vis.clear()\n",
    "vis.add(\"world\", world.world)\n",
    "\n",
    "# add table\n",
    "# the table are for visualization only, not added to the simulator to avoid extra computation in the simulation\n",
    "from klampt.model.create import box\n",
    "b1 = box(1.,1.5,1.1,center=(0.25,0,-0.55),type='GeometricPrimitive')\n",
    "vis.add(\"table\", b1, hide_label=True)\n",
    "b2 = box(0.7,0.45,0.12,center=(0.3,0.4,0),type='GeometricPrimitive')\n",
    "vis.add(\"box\", b2, hide_label=True)\n",
    "\n",
    "# add visualization object to show its estimated pose\n",
    "vis.add(\"object\", object)\n",
    "vis.show()\n",
    "\n",
    "import time\n",
    "for i in range(len(dataset)):\n",
    "    seq = dataset[i]\n",
    "    for frame in seq:\n",
    "        control = {}\n",
    "        control['kinova'] = frame['kinova_state']\n",
    "        control['punyo'] = frame['punyo_state']\n",
    "\n",
    "        # step simulation\n",
    "        vis.lock()\n",
    "        sim.step(control, 0.1)\n",
    "        object.setTransform(*se3.from_ndarray(frame['object_state'])) # object mesh is for visualization only, so not added to the simulator\n",
    "        vis.unlock()\n",
    "\n",
    "        time.sleep(2/len(seq)) # visualize each sequence for 2 second\n",
    "\n",
    "    sim.reset()\n",
    "    if i >= 5:\n",
    "        break\n",
    "vis.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SDF\n",
      "Computing SDF Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<vsf.sim.neural_vsf_body.NeuralVSFQuasistaticSimBody at 0x7f95267a1df0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create VSF model\n",
    "import torch\n",
    "from vsf.constructors import vsf_from_box, vsf_from_mesh, vsf_from_rgbd\n",
    "\n",
    "# create vsf from a bounding box\n",
    "# aabb = np.load(os.path.join(DATA_DIR, \"object\", \"aabb.npy\"))\n",
    "# vsf = vsf_from_box(aabb[0], aabb[1], type='neural')\n",
    "\n",
    "# create vsf from a mesh -- this will do a better job estimating stiffness at the object boundaries\n",
    "vsf = vsf_from_mesh(os.path.join(DATA_DIR, \"object\", \"mesh.obj\"))\n",
    "\n",
    "# create vsf from a heightmap\n",
    "# import imageio\n",
    "# rgb_image = imageio.imread(os.path.join(DATA_DIR, \"object\", \"color_img.jpg\"))\n",
    "# depth_image = imageio.imread(os.path.join(DATA_DIR, \"object\", \"depth_img.png\"))\n",
    "# depth_scale = 1000.0\n",
    "# depth_trunc = 2.0\n",
    "\n",
    "# intrinsic = json.load(open(os.path.join(DATA_DIR, \"object\", \"intrinsic.json\")))\n",
    "# intrinsic = np.array([[intrinsic['fx'], 0, intrinsic['cx']],\n",
    "#                       [0, intrinsic['fy'], intrinsic['cy']],\n",
    "#                       [0, 0, 1]])\n",
    "# extrinsic = json.load(open(os.path.join(DATA_DIR, \"object\", \"extrinsic.json\")))\n",
    "# extrinsic = np.array(extrinsic['cam2world'])\n",
    "# bmin, bmax = np.load(os.path.join(DATA_DIR, \"object\", \"aabb.npy\"))\n",
    "\n",
    "# vsf = vsf_from_rgbd(rgb_image, depth_image, bmin, bmax, intrinsic, extrinsic, depth_scale=depth_scale, depth_trunc=depth_trunc, type='neural')\n",
    "\n",
    "\n",
    "# move vsf to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vsf = vsf.to(device)\n",
    "\n",
    "# add vsf to the scene\n",
    "sim.add_deformable('object',vsf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run estimation\n",
    "\n",
    "The following code runs a batch estimation over the whole dataset.  Note that training is performed by randomizing over sequences in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dacite\n",
    "# from vsf.sensor.constructors import SensorConfig, CalibrationConfig, calibration_from_config, BaseCalibrator\n",
    "\n",
    "# calibrators_configs = {\n",
    "#     'kinova_joint_torques': {\n",
    "#         'type': 'tare',\n",
    "#         # 'num_samples': 20,\n",
    "#         'num_samples': 20,\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# calibrators = { k:calibration_from_config(dacite.from_dict(CalibrationConfig,v,config=dacite.Config(strict=True))) \n",
    "#                 for (k,v) in calibrators_configs.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch estimation, using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Batch VSF estimation demo\n",
    "from vsf.estimator.neural_vsf_estimator import NeuralVSFEstimator, NeuralVSFEstimatorConfig\n",
    "\n",
    "# create estimator\n",
    "estimator_config = NeuralVSFEstimatorConfig(lr=1e-3,\n",
    "                                            regularizer_samples=500,\n",
    "                                            regularizer_scale=1e-9,\n",
    "                                            max_epochs=500,\n",
    "                                            down_sample_rate=1)\n",
    "                                            \n",
    "estimator = NeuralVSFEstimator(estimator_config)\n",
    "\n",
    "print(\"Starting batch estimation, using device\",vsf.device)\n",
    "estimator.batch_estimate(sim, vsf, dataset, dataset_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code runs an online estimation for each sequence in the dataset.  This does have a risk of catastrophic forgetting, since there is no memory replay buffer (as in Point VSF online estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning sequence 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     control[k] \u001b[39m=\u001b[39m frame[control_keys[k]]\n\u001b[1;32m     27\u001b[0m sim\u001b[39m.\u001b[39mstep(control, dt)\n\u001b[0;32m---> 28\u001b[0m loss \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49monline_update(sim, dt, observation)\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m\"\u001b[39m,loss)\n",
      "File \u001b[0;32m~/vsf_release_workspace/openvsf/demos/../vsf/estimator/neural_vsf_estimator.py:67\u001b[0m, in \u001b[0;36mNeuralVSFEstimator.online_update\u001b[0;34m(self, sim, dt, observations)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39monline_update\u001b[39m(\u001b[39mself\u001b[39m, sim : QuasistaticVSFSimulator, dt, observations : Dict[\u001b[39mstr\u001b[39m,np\u001b[39m.\u001b[39mndarray]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m     66\u001b[0m     \u001b[39mfor\u001b[39;00m sensor \u001b[39min\u001b[39;00m sim\u001b[39m.\u001b[39msensors:\n\u001b[0;32m---> 67\u001b[0m         \u001b[39massert\u001b[39;00m sensor\u001b[39m.\u001b[39mname \u001b[39min\u001b[39;00m observations\n\u001b[1;32m     68\u001b[0m     \u001b[39mfor\u001b[39;00m sensor_name \u001b[39min\u001b[39;00m observations:\n\u001b[1;32m     69\u001b[0m         \u001b[39massert\u001b[39;00m sim\u001b[39m.\u001b[39mget_sensor(sensor_name) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Online VSF estimation demo\n",
    "\n",
    "# create estimator\n",
    "estimator_config = NeuralVSFEstimatorConfig(lr=1e-3,\n",
    "                                            regularizer_samples=500,\n",
    "                                            regularizer_scale=1e-9)\n",
    "estimator = NeuralVSFEstimator(estimator_config)\n",
    "\n",
    "estimator.online_init(sim, vsf)\n",
    "\n",
    "sensor_keys = dataset_config.sensor_keys\n",
    "control_keys = dataset_config.control_keys\n",
    "\n",
    "dt = 0.1\n",
    "for i in range(len(dataset)):\n",
    "    print(\"Beginning sequence\",i)\n",
    "    seq = dataset[i]\n",
    "    sim.reset()\n",
    "    estimator.online_reset(sim)\n",
    "    for frame in seq:\n",
    "        control, observation = {}, {}\n",
    "        for k in sensor_keys:\n",
    "            observation[k] = frame[sensor_keys[k]]\n",
    "        for k in control_keys:\n",
    "            control[k] = frame[control_keys[k]]\n",
    "\n",
    "        sim.step(control, dt)\n",
    "        loss = estimator.online_update(sim, dt, observation)\n",
    "        print(\"Loss\",loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vsf_show: Using the following stiffness values: [0.07224871776998043, 7420.5477900539445, 14841.023331390119, 22261.498872726293, 29681.974414062468]\n",
      "klampt.vis: auto-fitting camera to scene.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized OpenGL version\n",
      "Unrecognized OpenGL version\n"
     ]
    }
   ],
   "source": [
    "# visualize VSF\n",
    "from vsf.visualize.klampt_visualization import vsf_show\n",
    "vsf_show(vsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk        \n",
    "os.makedirs(os.path.join(DEMO_DIR, \"saved_vsfs\", OBJECT), exist_ok=True)\n",
    "vsf.save(os.path.join(DEMO_DIR, \"saved_vsfs\", OBJECT, \"neural_vsf_playground.pt\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural VSF to Point VSF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting voxel grid resolution 0.005341417590189599\n",
      "vsf_show: Using the following stiffness values: [1.1592156852202606e-06, 0.00021194749569986016, 0.00042273575672879815, 0.0006335240323096514, 0.0008443123078905046]\n",
      "klampt.vis: auto-fitting camera to scene.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized OpenGL version\n",
      "Unrecognized OpenGL version\n"
     ]
    }
   ],
   "source": [
    "from vsf.constructors import vsf_from_vsf, PointVSFConfig\n",
    "\n",
    "voxel_size = 3e-3\n",
    "point_config = PointVSFConfig(voxel_size=voxel_size)\n",
    "point_vsf = vsf_from_vsf(point_config, vsf)\n",
    "vsf_show(point_vsf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point VSF to Neural VSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxel size:  tensor(0.0030)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [05:08<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vsf_show: Using the following stiffness values: [0.043909826129674916, 5607.465451900842, 11214.886993975555, 16822.308536050266, 22429.73007812498]\n",
      "klampt.vis: auto-fitting camera to scene.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized OpenGL version\n",
      "Unrecognized OpenGL version\n"
     ]
    }
   ],
   "source": [
    "from vsf.constructors import vsf_from_vsf, NeuralVSFConfig\n",
    "\n",
    "neural_config = NeuralVSFConfig()\n",
    "\n",
    "convert_config = NeuralVSFEstimatorConfig(lr=2e-4,\n",
    "                                          regularizer_samples=1000,\n",
    "                                          regularizer_scale=1e2,\n",
    "                                          max_epochs=500)\n",
    "neural_vsf = vsf_from_vsf(neural_config, point_vsf, convert_config)\n",
    "vsf_show(neural_vsf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
