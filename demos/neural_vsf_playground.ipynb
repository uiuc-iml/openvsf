{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#if you haven't installed vsf and just want to run this from the current directory, uncomment the following lines\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the demo data from box (80MB): \n",
    "+ #### Demo data: https://uofi.box.com/s/31wozq63qgqvg8012r1jdfj5mdchmmfp\n",
    "\n",
    "### Demo data include 6 objects with heterogeneous stiffness:  \n",
    "\n",
    "- **brown_boot_moving**:  \n",
    "    - **Description**: a boot placed freely on the table. Punyo sensor + kinova robot arm is used to collect tactile data. Use FoundationPose to track 6D pose of the object.  \n",
    "    - **Support sensor(s)**: punyo dense force (suggested), punyo pressure, joint torque.  \n",
    "    - **VSF initialized from**: object mesh  \n",
    "- **brown_boot_fixed**:  \n",
    "    - **Description**: a boot mounted on the table. Punyo sensor + kinova robot arm is used to collect tactile data.  \n",
    "    - **Support sensor(s)**: punyo dense force (suggested), punyo pressure, joint torque.  \n",
    "    - **VSF initialized from**: object RGBD image (suggested), object bounding box  \n",
    "- **gray_shoe_fixed**:  \n",
    "    - same as above  \n",
    "- **white_nike_fixed**:  \n",
    "    - same as above  \n",
    "- **rubber_fig_tall_angle00**:  \n",
    "    - **Description**: an artificial tree placed on the ground. Use kinova arm to push the tree to collect the tactile data.  \n",
    "    - **Support sensor(s)**: joint torque.  \n",
    "    - **VSF initialized from**: object RGBD image  \n",
    "- **rubber_fig_tall_angle06**:  \n",
    "    - same as above  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize VSF\n",
    "\n",
    "We have already estimated some neural VSFs for you in the demo dataset.  To visualize them, you can use the `vsf_show` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# demo data directory\n",
    "DEMO_DIR = \"../demo_data\" # change to your path\n",
    "\n",
    "OBJECT = \"brown_boot_moving\" # choose from  \"brown_boot_moving\", \"brown_boot_fixed\", \"rubber_fig_tall_angle00\", ...\n",
    "DATA_DIR = os.path.join(DEMO_DIR, 'datasets', OBJECT)\n",
    "TRIAL = \"\"\n",
    "\n",
    "SENSOR = \"PunyoDenseForceSensor\" # choose from PunyoDenseForceSensor, PunyoPressureSensor, JointTorqueSensor\n",
    "INIT_METHOD = \"mesh\" # choose from mesh, rgbd, aabb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# visualize VSF\n",
    "from vsf.constructors import vsf_from_file\n",
    "import torch\n",
    "\n",
    "#estimated VSFs have been saved to the \"outputs\" folder\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# vsf = vsf_from_file(os.path.join(DEMO_DIR, \"saved_vsfs\", OBJECT, \"neural_vsf.pt\")).to(device)\n",
    "\n",
    "from klampt import vis\n",
    "from vsf.visualize.klampt_visualization import vsf_show\n",
    "# vis.init('PyQt')  #needed inside Jupyter Notebook to show an OpenGL window\n",
    "# vsf_show(vsf)\n",
    "# vis.clear()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walkthrough neural VSF training and visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a world and simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WorldModel::LoadRobot: ../knowledge/robot_model/kinova_gen3.urdf\n",
      "URDFParser: Link size: 9\n",
      "URDFParser: Joint size: 9\n",
      "URDFParser: Done loading robot file ../knowledge/robot_model/kinova_gen3.urdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9936/747523783.py:38: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  rgb_image = imageio.imread(os.path.join(DATA_DIR, \"object\", \"color_img.jpg\"))\n",
      "/tmp/ipykernel_9936/747523783.py:39: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  depth_image = imageio.imread(os.path.join(DATA_DIR, \"object\", \"depth_img.png\"))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import klampt\n",
    "from klampt.math import se3\n",
    "from vsf.sim import QuasistaticVSFSimulator\n",
    "from vsf.sensor.joint_torque_sensor import JointTorqueSensor\n",
    "from vsf.sensor.punyo_pressure_sensor import PunyoPressureSensor\n",
    "from vsf.sensor.punyo_dense_force_sensor import PunyoDenseForceSensor\n",
    "from vsf.sim.klampt_world_wrapper import klamptWorldWrapper\n",
    "from vsf.utils.klampt_utils import load_trimesh_preserve_vertices\n",
    "\n",
    "# NOTE: This part of the code shows how to manually setup a simulation world with a robot, a tactile sensor and a deformable object,\n",
    "#       and how to estimate the VSF from tactile data.\n",
    "#       A script to automate this process from config files is in scripts/neural_vsf_estimate.py\n",
    "\n",
    "# create a world for simulation and visualization\n",
    "world = klamptWorldWrapper()\n",
    "\n",
    "# add robot/mesh to the world\n",
    "world.add_robot('kinova', os.path.join('../knowledge/robot_model', 'kinova_gen3.urdf'))\n",
    "robot = world.world.robot(0)\n",
    "ee_link = robot.link(robot.numLinks()-1)\n",
    "punyo2ee = np.array(json.load(open(os.path.join('../knowledge/robot_model', 'punyo2end_effector_transform.json')))['punyo2ee'])\n",
    "\n",
    "#this function should be used instead of native Klampt loaders due to a known Assimp configuration issue\n",
    "mesh = load_trimesh_preserve_vertices(os.path.join('../knowledge/robot_model', 'punyo_mesh_partial.ply'))\n",
    "world.add_geometry('punyo', mesh, 'deformable', ee_link.getName(), punyo2ee)\n",
    "\n",
    "# adds the VSF object to the world, for visualization purposes\n",
    "# hack for add object to the visualization without interfering with the simulator\n",
    "world2 = klampt.WorldModel()\n",
    "object = world2.makeRigidObject(\"object\")\n",
    "if os.path.exists(os.path.join(DATA_DIR, \"object\", \"mesh.obj\")):\n",
    "    object.geometry().loadFile(os.path.join(DATA_DIR, \"object\", \"mesh.obj\"))\n",
    "elif os.path.exists(os.path.join(DATA_DIR, \"object\", \"color_img.jpg\")) and os.path.exists(os.path.join(DATA_DIR, \"object\", \"depth_img.png\")):\n",
    "    # visualize the object with a heightmap (projected from RGB-D images)\n",
    "    import imageio\n",
    "    rgb_image = imageio.imread(os.path.join(DATA_DIR, \"object\", \"color_img.jpg\"))\n",
    "    depth_image = imageio.imread(os.path.join(DATA_DIR, \"object\", \"depth_img.png\"))\n",
    "    depth_scale = 1000.0\n",
    "    depth_trunc = 2.0\n",
    "\n",
    "    intrinsic = json.load(open(os.path.join(DATA_DIR, \"object\", \"intrinsic.json\")))\n",
    "    extrinsic = json.load(open(os.path.join(DATA_DIR, \"object\", \"extrinsic.json\")))\n",
    "    extrinsic = np.array(extrinsic['cam2world'])\n",
    "\n",
    "    bmin, bmax = np.load(os.path.join(DATA_DIR, \"object\", \"aabb.npy\"))\n",
    "    vp = klampt.Viewport()\n",
    "    vp.setPose(*se3.from_homogeneous(extrinsic))\n",
    "    vp.w, vp.h = rgb_image.shape[1], rgb_image.shape[0]\n",
    "    vp.fx, vp.fy = intrinsic['fx'], intrinsic['fy']\n",
    "    vp.cx, vp.cy = intrinsic['cx'], intrinsic['cy']\n",
    "\n",
    "    # remove points beyond the truncation distance    \n",
    "    depth_image[depth_image > depth_trunc*depth_scale] = 0\n",
    "\n",
    "    # create a heightmap from the rgbd image\n",
    "    hm_data = klampt.Heightmap()\n",
    "    hm_data.setViewport(vp)\n",
    "    hm_data.setHeightImage(depth_image, 1/depth_scale)\n",
    "    hm_data.setColorImage(rgb_image)\n",
    "    object.geometry().setHeightmap(hm_data)\n",
    "    \n",
    "else:\n",
    "    aabb = np.load(os.path.join(DATA_DIR, \"object\", \"aabb.npy\"))\n",
    "    object.geometry().setTriangleMesh(klampt.model.create.primitives.bbox(*aabb).getTriangleMesh())\n",
    "    \n",
    "\n",
    "# initialize sensors\n",
    "if SENSOR == \"PunyoDenseForceSensor\":\n",
    "    sensors = [PunyoDenseForceSensor('punyo_force', 'punyo')] # add sensor named 'punyo_force' and attach it to the mesh named 'punyo'\n",
    "elif SENSOR == \"PunyoPressureSensor\":\n",
    "    sensors = [PunyoPressureSensor('punyo_pressure', 'punyo')]\n",
    "elif SENSOR == \"JointTorqueSensor\":\n",
    "    sensors = [JointTorqueSensor('kinova_joint_torques','kinova',[robot.link(i).name for i in range(1,8)])]\n",
    "\n",
    "# create simulator\n",
    "sim = QuasistaticVSFSimulator(world, sensors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tactile dataset and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dacite\n",
    "from vsf.dataset.constructors import dataset_from_config, DatasetConfig\n",
    "\n",
    "from vsf.utils.config_utils import load_config_recursive\n",
    "if SENSOR == \"JointTorqueSensor\":\n",
    "    dataset_config = load_config_recursive(os.path.join('../demo_data/datasets/joint_torques_dataset_config.yaml'))\n",
    "else:\n",
    "    dataset_config = load_config_recursive(os.path.join('../demo_data/datasets/punyo_dataset_config.yaml'))\n",
    "dataset_config = dacite.from_dict(DatasetConfig, dataset_config)\n",
    "dataset_config.path = os.path.join('../demo_data/datasets', OBJECT, TRIAL)\n",
    "dataset = dataset_from_config(dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***  klampt.vis: using Qt6 as the visualization backend  ***\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: QApplication was not created in the main() thread.\n",
      "inotify_add_watch(/home/motion/.config/ibus/bus/260a30b2eed9481b9e0a65be0d7891d6-unix-1) failed: (No space left on device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vis: creating GL window\n",
      "######### QtGLWindow setProgram ###############\n",
      "#########################################\n",
      "klampt.vis: Making window 0\n",
      "#########################################\n",
      "######### QtGLWindow Initialize GL ###############\n",
      "QtGLWindow.resizeGL: called when invisible?\n",
      "QtGLWindow.paintGL: called while invisible?\n",
      "TriMeshTopology: mesh has 9 triangles with duplicate neighbors!\n",
      "  Triangle range 273 to 924\n",
      "  May see strange results for some triangle mesh operations\n",
      "TriMeshTopology: mesh has 104 triangles with duplicate neighbors!\n",
      "  Triangle range 3828 to 4274\n",
      "  May see strange results for some triangle mesh operations\n",
      "TriMeshTopology: mesh has 271 triangles with duplicate neighbors!\n",
      "  Triangle range 80 to 3362\n",
      "  May see strange results for some triangle mesh operations\n",
      "TriMeshTopology: mesh has 635 triangles with duplicate neighbors!\n",
      "  Triangle range 29 to 3626\n",
      "  May see strange results for some triangle mesh operations\n",
      "TriMeshTopology: mesh has 587 triangles with duplicate neighbors!\n",
      "  Triangle range 1988 to 3472\n",
      "  May see strange results for some triangle mesh operations\n",
      "TriMeshTopology: mesh has 344 triangles with duplicate neighbors!\n",
      "  Triangle range 4000 to 5015\n",
      "  May see strange results for some triangle mesh operations\n",
      "TriMeshTopology: mesh has 344 triangles with duplicate neighbors!\n",
      "  Triangle range 3199 to 4281\n",
      "  May see strange results for some triangle mesh operations\n",
      "TriMeshTopology: mesh has 378 triangles with duplicate neighbors!\n",
      "  Triangle range 2 to 8834\n",
      "  May see strange results for some triangle mesh operations\n",
      "GeometryAppearance: preprocessing mesh with 1196880 tris took 1.34671s, try disabling creasing and silhouettes\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_001\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_002\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_003\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_004\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_005\n"
     ]
    }
   ],
   "source": [
    "from klampt import vis\n",
    "vis.init('PyQt')  #needed inside Jupyter Notebook to show an OpenGL window\n",
    "vis.clear()\n",
    "vis.add(\"world\", world.world)\n",
    "\n",
    "# add table\n",
    "# the table are for visualization only, not added to the simulator to avoid extra computation in the simulation\n",
    "from klampt.model.create import box\n",
    "b1 = box(1.,1.5,1.1,center=(0.25,0,-0.55),type='GeometricPrimitive')\n",
    "vis.add(\"table\", b1, hide_label=True)\n",
    "b2 = box(0.7,0.45,0.12,center=(0.3,0.4,0),type='GeometricPrimitive')\n",
    "vis.add(\"box\", b2, hide_label=True)\n",
    "\n",
    "# add visualization object to show its estimated pose\n",
    "vis.add(\"object\", object)\n",
    "vis.show()\n",
    "\n",
    "import time\n",
    "for i in range(len(dataset)):\n",
    "    seq = dataset[i]\n",
    "    for frame in seq:\n",
    "        control = {}\n",
    "        control['kinova'] = frame['kinova_state']\n",
    "        control['punyo'] = frame['punyo_state']\n",
    "\n",
    "        # step simulation\n",
    "        vis.lock()\n",
    "        sim.step(control, 0.1)\n",
    "        object.setTransform(*se3.from_ndarray(frame['object_state'])) # object mesh is for visualization only, so not added to the simulator\n",
    "        vis.unlock()\n",
    "\n",
    "        time.sleep(2/len(seq)) # visualize each sequence for 2 second\n",
    "\n",
    "    sim.reset()\n",
    "    if i >= 5:\n",
    "        break\n",
    "vis.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<vsf.sim.neural_vsf_body.NeuralVSFQuasistaticSimBody at 0x7fb8feecf790>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create VSF model\n",
    "import torch\n",
    "from vsf.constructors import vsf_from_box, vsf_from_mesh, vsf_from_rgbd\n",
    "\n",
    "if INIT_METHOD == \"aabb\":\n",
    "    # create vsf from a bounding box\n",
    "    aabb = np.load(os.path.join(DATA_DIR, \"object\", \"aabb.npy\"))\n",
    "    vsf = vsf_from_box(aabb[0], aabb[1], type='neural')\n",
    "elif INIT_METHOD == \"mesh\":\n",
    "    # create vsf from a mesh -- this will do a better job estimating stiffness at the object boundaries\n",
    "    vsf = vsf_from_mesh(os.path.join(DATA_DIR, \"object\", \"mesh.obj\"))\n",
    "elif INIT_METHOD == \"rgbd\":\n",
    "    # create vsf from a rgbd images\n",
    "    import imageio\n",
    "    rgb_image = imageio.imread(os.path.join(DATA_DIR, \"object\", \"color_img.jpg\"))\n",
    "    depth_image = imageio.imread(os.path.join(DATA_DIR, \"object\", \"depth_img.png\"))\n",
    "    depth_scale = 1000.0\n",
    "    depth_trunc = 2.0\n",
    "\n",
    "    intrinsic = json.load(open(os.path.join(DATA_DIR, \"object\", \"intrinsic.json\")))\n",
    "    intrinsic = np.array([[intrinsic['fx'], 0, intrinsic['cx']],\n",
    "                          [0, intrinsic['fy'], intrinsic['cy']],\n",
    "                          [0, 0, 1]])\n",
    "    extrinsic = json.load(open(os.path.join(DATA_DIR, \"object\", \"extrinsic.json\")))\n",
    "    extrinsic = np.array(extrinsic['cam2world'])\n",
    "    bmin, bmax = np.load(os.path.join(DATA_DIR, \"object\", \"aabb.npy\"))\n",
    "\n",
    "    vsf = vsf_from_rgbd(rgb_image, depth_image, bmin, bmax, intrinsic, extrinsic, depth_scale=depth_scale, depth_trunc=depth_trunc, type='neural')\n",
    "\n",
    "\n",
    "# move vsf to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vsf = vsf.to(device)\n",
    "\n",
    "# add vsf to the scene\n",
    "sim.add_deformable('object',vsf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run estimation\n",
    "\n",
    "The following code runs a batch estimation over the whole dataset.  Note that training is performed by randomizing over sequences in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SENSOR == \"PunyoDenseForceSensor\":\n",
    "    calibrators = {}\n",
    "else:\n",
    "    import dacite\n",
    "    from vsf.sensor.constructors import SensorConfig, CalibrationConfig, calibration_from_config, BaseCalibrator\n",
    "\n",
    "    calibrators_configs = {\n",
    "        'kinova_joint_torques': {\n",
    "            'type': 'tare',\n",
    "            'num_samples': 20,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    calibrators = { k:calibration_from_config(dacite.from_dict(CalibrationConfig,v,config=dacite.Config(strict=True))) \n",
    "                    for (k,v) in calibrators_configs.items() }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch estimation, using device cuda:0\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_040\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_066\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_075\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_048\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_062\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_052\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_041\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_057\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_030\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_072\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_055\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_024\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_045\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_009\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_027\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_078\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_011\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_082\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_014\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_073\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_087\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_047\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_008\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_039\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_081\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_086\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_017\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_049\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_022\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_074\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_021\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_035\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_063\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_051\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_059\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_050\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_033\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_067\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_038\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_058\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_077\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_012\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_076\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_071\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_083\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_015\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_010\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_020\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_007\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_046\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_036\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_034\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_079\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_085\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_054\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_032\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_006\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_044\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_029\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_061\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_016\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_042\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_053\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_026\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_056\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_064\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_080\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_084\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_060\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_069\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_043\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_019\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_013\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_023\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_025\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_031\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_070\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_065\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_037\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_068\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_018\n",
      "Loading sequence ../demo_data/datasets/brown_boot_fixed/seq_028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################\n",
      "klampt.vis: Window 0 close\n",
      "#########################################\n"
     ]
    }
   ],
   "source": [
    "# Batch VSF estimation demo\n",
    "from vsf.estimator.neural_vsf_estimator import NeuralVSFEstimator, NeuralVSFEstimatorConfig\n",
    "\n",
    "# downsample for joint torque sensor\n",
    "if SENSOR == \"JointTorqueSensor\":\n",
    "    down_sample_rate = 10\n",
    "else:\n",
    "    down_sample_rate = 1\n",
    "\n",
    "# create estimator\n",
    "estimator_config = NeuralVSFEstimatorConfig(lr=2e-4,\n",
    "                                            regularizer_samples=500,\n",
    "                                            regularizer_scale=1e-9,\n",
    "                                            max_epochs=500,\n",
    "                                            down_sample_rate=down_sample_rate)\n",
    "                                            \n",
    "estimator = NeuralVSFEstimator(estimator_config)\n",
    "\n",
    "print(\"Starting batch estimation, using device\",vsf.device)\n",
    "estimator.batch_estimate(sim, vsf, dataset, dataset_config, calibrators)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code runs an online estimation for each sequence in the dataset.  This does have a risk of catastrophic forgetting, since there is no memory replay buffer (as in Point VSF online estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online VSF estimation demo\n",
    "from vsf.utils.data_utils import remap_dict_in_seq\n",
    "# create estimator\n",
    "estimator_config = NeuralVSFEstimatorConfig(lr=1e-3,\n",
    "                                            regularizer_samples=500,\n",
    "                                            regularizer_scale=1e-9,\n",
    "                                            down_sample_rate=down_sample_rate)\n",
    "estimator = NeuralVSFEstimator(estimator_config)\n",
    "\n",
    "estimator.online_init(sim, vsf)\n",
    "\n",
    "dt = 0.1\n",
    "for i in range(len(dataset)):\n",
    "    print(\"Beginning sequence\",i)\n",
    "    seq = dataset[i]\n",
    "    sim.reset()\n",
    "    estimator.online_reset(sim)\n",
    "\n",
    "    control_keys = sim.get_control_keys()\n",
    "    sensor_keys = sim.get_sensor_keys()\n",
    "    control_seq, observation_seq = remap_dict_in_seq(seq, control_keys, sensor_keys)\n",
    "\n",
    "    #calibrate the sensors from the sequences\n",
    "    ncalibrate = 0\n",
    "    if calibrators is not None and calibrators != {}:\n",
    "        for k,v in calibrators.items():\n",
    "            sensor = sim.get_sensor(k)\n",
    "            if sensor is None: raise ValueError(f\"Sensor {k} not found in simulator\")\n",
    "            n = v.calibrate(sensor, sim, control_seq, observation_seq)\n",
    "            ncalibrate = max(n,ncalibrate)\n",
    "\n",
    "\n",
    "    for i in range(ncalibrate, len(seq), estimator_config.down_sample_rate):\n",
    "        controls = control_seq[i]\n",
    "        observations = observation_seq[i]\n",
    "        sim.step(controls,dt)\n",
    "    \n",
    "        estimator.online_update(sim, dt, observations)\n",
    "\n",
    "estimator.online_finalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize VSF\n",
    "from vsf.visualize.klampt_visualization import vsf_show\n",
    "vsf_show(vsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk        \n",
    "os.makedirs(os.path.join(DEMO_DIR, \"saved_vsfs\", OBJECT), exist_ok=True)\n",
    "vsf.save(os.path.join(DEMO_DIR, \"saved_vsfs\", OBJECT, \"neural_vsf_playground.pt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural VSF to Point VSF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vsf.constructors import vsf_from_vsf, PointVSFConfig\n",
    "\n",
    "voxel_size = 3e-3\n",
    "point_config = PointVSFConfig(voxel_size=voxel_size)\n",
    "point_vsf = vsf_from_vsf(point_config, vsf)\n",
    "vsf_show(point_vsf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point VSF to Neural VSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vsf.constructors import vsf_from_vsf, NeuralVSFConfig\n",
    "\n",
    "neural_config = NeuralVSFConfig()\n",
    "\n",
    "convert_config = NeuralVSFEstimatorConfig(lr=2e-4,\n",
    "                                          regularizer_samples=1000,\n",
    "                                          regularizer_scale=1e2,\n",
    "                                          max_epochs=500)\n",
    "neural_vsf = vsf_from_vsf(neural_config, point_vsf, convert_config)\n",
    "vsf_show(neural_vsf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
